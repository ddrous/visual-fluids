{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b16dfc76-0c78-416c-9565-c0825087a06a",
   "metadata": {},
   "source": [
    "# Neural ODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8768437-4934-45d8-9d34-d0783e81cf94",
   "metadata": {},
   "source": [
    "This example trains a [Neural ODE](https://arxiv.org/abs/1806.07366) to reproduce a toy dataset of nonlinear oscillators.\n",
    "\n",
    "This example is available as a Jupyter notebook [here](https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_ode.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "276cbbe5-dac1-4814-807c-e50cc633b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f36a4f-3813-4f84-8d61-96500ea1f237",
   "metadata": {},
   "source": [
    "We use [Equinox](https://github.com/patrick-kidger/equinox) to build neural networks. We use [Optax](https://github.com/deepmind/optax) for optimisers (Adam etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5016bd6c-4981-4783-a3cf-087b89b680c4",
   "metadata": {},
   "source": [
    "Recalling that a neural ODE is defined as\n",
    "\n",
    "$y(t) = y(0) + \\int_0^t f_\\theta(s, y(s)) ds$,\n",
    "\n",
    "then here we're now about to define the $f_\\theta$ that appears on that right hand side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "243412cd-9f19-489f-a10e-bf0eb8bf3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    mlp: eqx.nn.MLP\n",
    "    a: jnp.ndarray\n",
    "    b: jnp.ndarray\n",
    "\n",
    "    def __init__(self, data_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=data_size,\n",
    "            out_size=data_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=jnn.softplus,\n",
    "            key=key,\n",
    "        )\n",
    "        new_key = jax.random.PRNGKey(0)\n",
    "        a_k, b_k = jax.random.split(new_key, 2)\n",
    "        self.a = jax.random.normal(a_k, (2,))\n",
    "        self.b = jax.random.normal(b_k, (2,))\n",
    "\n",
    "    def __call__(self, t, y, args):\n",
    "        # print(\"current shapes:\", y.shape, self.a.shape)\n",
    "        # print(\"current values:\", self.a, self.b)\n",
    "        return self.a*y / (self.b + y)\n",
    "        return self.mlp(y) + rhs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa17137-a851-4214-9187-53711d0e07da",
   "metadata": {},
   "source": [
    "Here we wrap up the entire ODE solve into a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fdb14ae0-1aa5-4e10-ba3b-d977d5d6ac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralODE(eqx.Module):\n",
    "    func: Func\n",
    "\n",
    "    def __init__(self, data_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.func = Func(data_size, width_size, depth, key=key)\n",
    "\n",
    "    def __call__(self, ts, y0):\n",
    "        solution = diffrax.diffeqsolve(\n",
    "            diffrax.ODETerm(self.func),\n",
    "            diffrax.Tsit5(),\n",
    "            t0=ts[0],\n",
    "            t1=ts[-1],\n",
    "            dt0=ts[1] - ts[0],\n",
    "            y0=y0,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n",
    "            saveat=diffrax.SaveAt(ts=ts),\n",
    "        )\n",
    "        return solution.ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d3264c-2edc-465e-b61f-05125645e1df",
   "metadata": {},
   "source": [
    "Toy dataset of nonlinear oscillators. Sample paths look like deformed sines and cosines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "29225b09-3a50-4a7f-bbcf-5824d44f3e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_data(ts, *, key):\n",
    "    y0 = jrandom.uniform(key, (2,), minval=-0.6, maxval=1)\n",
    "\n",
    "    def f(t, y, args):\n",
    "        x = y / (1 + y)\n",
    "        return jnp.stack([x[1], -x[0]], axis=-1)\n",
    "\n",
    "    solver = diffrax.Tsit5()\n",
    "    dt0 = 0.1\n",
    "    saveat = diffrax.SaveAt(ts=ts)\n",
    "    sol = diffrax.diffeqsolve(\n",
    "        diffrax.ODETerm(f), solver, ts[0], ts[-1], dt0, y0, saveat=saveat\n",
    "    )\n",
    "    ys = sol.ys\n",
    "    return ys\n",
    "\n",
    "\n",
    "def get_data(dataset_size, *, key):\n",
    "    ts = jnp.linspace(0, 10, 100)\n",
    "    key = jrandom.split(key, dataset_size)\n",
    "    ys = jax.vmap(lambda key: _get_data(ts, key=key))(key)\n",
    "    return ts, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6505387d-6900-401d-9ceb-b741f349f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd937f6-4642-4b8a-b309-aae63a9269dc",
   "metadata": {},
   "source": [
    "Main entry point. Try runnning `main()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "135540f6-d5ea-4c79-b083-0b86fd3edbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=256,\n",
    "    batch_size=32,\n",
    "    lr_strategy=(3e-3, 3e-3),\n",
    "    steps_strategy=(500, 500),\n",
    "    length_strategy=(0.1, 1),\n",
    "    width_size=64,\n",
    "    depth=2,\n",
    "    seed=5678,\n",
    "    plot=True,\n",
    "    print_every=100,\n",
    "):\n",
    "    key = jrandom.PRNGKey(seed)\n",
    "    data_key, model_key, loader_key = jrandom.split(key, 3)\n",
    "\n",
    "    ts, ys = get_data(dataset_size, key=data_key)\n",
    "    _, length_size, data_size = ys.shape\n",
    "\n",
    "    model = NeuralODE(data_size, width_size, depth, key=model_key)\n",
    "\n",
    "    # Training loop like normal.\n",
    "    #\n",
    "    # Only thing to notice is that up until step 500 we train on only the first 10% of\n",
    "    # each time series. This is a standard trick to avoid getting caught in a local\n",
    "    # minimum.\n",
    "\n",
    "    @eqx.filter_value_and_grad\n",
    "    def grad_loss(model, ti, yi):\n",
    "        y_pred = jax.vmap(model, in_axes=(None, 0))(ti, yi[:, 0])\n",
    "        return jnp.mean((yi - y_pred) ** 2)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(ti, yi, model, opt_state):\n",
    "        loss, grads = grad_loss(model, ti, yi)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return loss, model, opt_state\n",
    "\n",
    "    for lr, steps, length in zip(lr_strategy, steps_strategy, length_strategy):\n",
    "        optim = optax.adabelief(lr)\n",
    "        opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "        _ts = ts[: int(length_size * length)]\n",
    "        _ys = ys[:, : int(length_size * length)]\n",
    "        for step, (yi,) in zip(\n",
    "            range(steps), dataloader((_ys,), batch_size, key=loader_key)\n",
    "        ):\n",
    "            start = time.time()\n",
    "            loss, model, opt_state = make_step(_ts, yi, model, opt_state)\n",
    "            end = time.time()\n",
    "            if (step % print_every) == 0 or step == steps - 1:\n",
    "                print(f\"Step: {step}, Loss: {loss}, Computation time: {end - start}\")\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(ts, ys[0, :, 0], c=\"dodgerblue\", label=\"Real\")\n",
    "        plt.plot(ts, ys[0, :, 1], c=\"dodgerblue\")\n",
    "        model_y = model(ts, ys[0, 0])\n",
    "        plt.plot(ts, model_y[:, 0], c=\"crimson\", label=\"Model\")\n",
    "        plt.plot(ts, model_y[:, 1], c=\"crimson\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"temp/neural_ode.png\")\n",
    "        plt.show()\n",
    "\n",
    "    return ts, ys, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fba5156d-22d4-40c9-991b-e6b29a53abed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current shapes: (2,) (2,)\n",
      "current values: Traced<ShapedArray(float32[2])>with<JVPTrace(level=2/2)> with\n",
      "  primal = Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace(level=0/2)>\n",
      "  tangent = Traced<ShapedArray(float32[2])>with<JaxprTrace(level=1/2)> with\n",
      "    pval = (ShapedArray(float32[2]), None)\n",
      "    recipe = LambdaBinding() Traced<ShapedArray(float32[2])>with<JVPTrace(level=2/2)> with\n",
      "  primal = Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace(level=0/2)>\n",
      "  tangent = Traced<ShapedArray(float32[2])>with<JaxprTrace(level=1/2)> with\n",
      "    pval = (ShapedArray(float32[2]), None)\n",
      "    recipe = LambdaBinding()\n",
      "current shapes: (2,) (2,)\n",
      "current values: Traced<ShapedArray(float32[2])>with<JVPTrace(level=2/2)> with\n",
      "  primal = Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace(level=0/2)>\n",
      "  tangent = Traced<ShapedArray(float32[2])>with<JaxprTrace(level=1/2)> with\n",
      "    pval = (ShapedArray(float32[2]), None)\n",
      "    recipe = LambdaBinding() Traced<ShapedArray(float32[2])>with<JVPTrace(level=2/2)> with\n",
      "  primal = Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace(level=0/2)>\n",
      "  tangent = Traced<ShapedArray(float32[2])>with<JaxprTrace(level=1/2)> with\n",
      "    pval = (ShapedArray(float32[2]), None)\n",
      "    recipe = LambdaBinding()\n",
      "current shapes: (2,) (2,)\n",
      "current values: Traced<ShapedArray(float32[2])>with<JVPTrace(level=2/2)> with\n",
      "  primal = Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace(level=0/2)>\n",
      "  tangent = Traced<ShapedArray(float32[2])>with<JaxprTrace(level=1/2)> with\n",
      "    pval = (ShapedArray(float32[2]), None)\n",
      "    recipe = LambdaBinding() Traced<ShapedArray(float32[2])>with<JVPTrace(level=2/2)> with\n",
      "  primal = Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace(level=0/2)>\n",
      "  tangent = Traced<ShapedArray(float32[2])>with<JaxprTrace(level=1/2)> with\n",
      "    pval = (ShapedArray(float32[2]), None)\n",
      "    recipe = LambdaBinding()\n",
      "current shapes: (2,) (2,)\n",
      "current values: Traced<ShapedArray(float32[2])>with<JVPTrace(level=2/2)> with\n",
      "  primal = Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace(level=0/2)>\n",
      "  tangent = Traced<ShapedArray(float32[2])>with<JaxprTrace(level=1/2)> with\n",
      "    pval = (ShapedArray(float32[2]), None)\n",
      "    recipe = LambdaBinding() Traced<ShapedArray(float32[2])>with<JVPTrace(level=2/2)> with\n",
      "  primal = Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace(level=0/2)>\n",
      "  tangent = Traced<ShapedArray(float32[2])>with<JaxprTrace(level=1/2)> with\n",
      "    pval = (ShapedArray(float32[2]), None)\n",
      "    recipe = LambdaBinding()\n",
      "current shapes: (2,) (2,)\n",
      "current values: Traced<ShapedArray(float32[2])>with<JVPTrace(level=2/2)> with\n",
      "  primal = Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace(level=0/2)>\n",
      "  tangent = Traced<ShapedArray(float32[2])>with<JaxprTrace(level=1/2)> with\n",
      "    pval = (ShapedArray(float32[2]), None)\n",
      "    recipe = LambdaBinding() Traced<ShapedArray(float32[2])>with<JVPTrace(level=2/2)> with\n",
      "  primal = Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace(level=0/2)>\n",
      "  tangent = Traced<ShapedArray(float32[2])>with<JaxprTrace(level=1/2)> with\n",
      "    pval = (ShapedArray(float32[2]), None)\n",
      "    recipe = LambdaBinding()\n",
      "current shapes: (2,) (2,)\n",
      "current values: Traced<ShapedArray(float32[2])>with<JVPTrace(level=2/2)> with\n",
      "  primal = Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace(level=0/2)>\n",
      "  tangent = Traced<ShapedArray(float32[2])>with<JaxprTrace(level=1/2)> with\n",
      "    pval = (ShapedArray(float32[2]), None)\n",
      "    recipe = LambdaBinding() Traced<ShapedArray(float32[2])>with<JVPTrace(level=2/2)> with\n",
      "  primal = Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace(level=0/2)>\n",
      "  tangent = Traced<ShapedArray(float32[2])>with<JaxprTrace(level=1/2)> with\n",
      "    pval = (ShapedArray(float32[2]), None)\n",
      "    recipe = LambdaBinding()\n",
      "current shapes: (2,) (2,)\n",
      "current values: Traced<ShapedArray(float32[2])>with<JVPTrace(level=2/2)> with\n",
      "  primal = Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace(level=0/2)>\n",
      "  tangent = Traced<ShapedArray(float32[2])>with<JaxprTrace(level=1/2)> with\n",
      "    pval = (ShapedArray(float32[2]), None)\n",
      "    recipe = LambdaBinding() Traced<ShapedArray(float32[2])>with<JVPTrace(level=2/2)> with\n",
      "  primal = Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace(level=0/2)>\n",
      "  tangent = Traced<ShapedArray(float32[2])>with<JaxprTrace(level=1/2)> with\n",
      "    pval = (ShapedArray(float32[2]), None)\n",
      "    recipe = LambdaBinding()\n",
      "Step: 0, Loss: 0.08727002888917923, Computation time: 6.873931884765625\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "INTERNAL: Generated function failed: CpuCallback error: RuntimeError: The maximum number of solver steps was reached. Try increasing `max_steps`.\n\nAt:\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/equinox/internal/errors.py(17): raises\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/jax/_src/callback.py(124): _flat_callback\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/jax/_src/callback.py(41): pure_callback_impl\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/jax/_src/callback.py(104): _callback\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/jax/interpreters/mlir.py(1579): _wrapped_callback\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/equinox/jit.py(78): _fun_wrapper\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/equinox/jit.py(82): __call__\n  /var/folders/fm/dr9q5mc50ql8h2wmv7b1fmz80000gn/T/ipykernel_78188/2677578158.py(48): main\n  /var/folders/fm/dr9q5mc50ql8h2wmv7b1fmz80000gn/T/ipykernel_78188/1014090899.py(1): <module>\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3433): run_code\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3373): run_ast_nodes\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3194): run_cell_async\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/IPython/core/async_helpers.py(129): _pseudo_sync_runner\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py(2995): _run_cell\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py(2940): run_cell\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/ipykernel/zmqshell.py(528): run_cell\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/ipykernel/ipkernel.py(383): do_execute\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/ipykernel/kernelbase.py(730): execute_request\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/ipykernel/kernelbase.py(406): dispatch_shell\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/ipykernel/kernelbase.py(499): process_one\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/ipykernel/kernelbase.py(510): dispatch_queue\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/asyncio/events.py(80): _run\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/asyncio/base_events.py(1899): _run_once\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/asyncio/base_events.py(603): run_forever\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/tornado/platform/asyncio.py(215): start\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/ipykernel/kernelapp.py(712): start\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/traitlets/config/application.py(846): launch_instance\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/ipykernel_launcher.py(17): <module>\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/runpy.py(86): _run_code\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/runpy.py(196): _run_module_as_main\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ts, ys, model \u001b[39m=\u001b[39m main()\n",
      "Cell \u001b[0;32mIn [64], line 48\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(dataset_size, batch_size, lr_strategy, steps_strategy, length_strategy, width_size, depth, seed, plot, print_every)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mfor\u001b[39;00m step, (yi,) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[1;32m     45\u001b[0m     \u001b[39mrange\u001b[39m(steps), dataloader((_ys,), batch_size, key\u001b[39m=\u001b[39mloader_key)\n\u001b[1;32m     46\u001b[0m ):\n\u001b[1;32m     47\u001b[0m     start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 48\u001b[0m     loss, model, opt_state \u001b[39m=\u001b[39m make_step(_ts, yi, model, opt_state)\n\u001b[1;32m     49\u001b[0m     end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     50\u001b[0m     \u001b[39mif\u001b[39;00m (step \u001b[39m%\u001b[39m print_every) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m step \u001b[39m==\u001b[39m steps \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/jaxenv/lib/python3.10/site-packages/equinox/jit.py:82\u001b[0m, in \u001b[0;36m_JitWrapper.__call__\u001b[0;34m(_JitWrapper__self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(__self, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mreturn\u001b[39;00m __self\u001b[39m.\u001b[39;49m_fun_wrapper(\u001b[39mFalse\u001b[39;49;00m, args, kwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/jaxenv/lib/python3.10/site-packages/equinox/jit.py:78\u001b[0m, in \u001b[0;36m_JitWrapper._fun_wrapper\u001b[0;34m(self, is_lower, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached\u001b[39m.\u001b[39mlower(dynamic, static)\n\u001b[1;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     dynamic_out, static_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cached(dynamic, static)\n\u001b[1;32m     79\u001b[0m     \u001b[39mreturn\u001b[39;00m combine(dynamic_out, static_out\u001b[39m.\u001b[39mvalue)\n",
      "\u001b[0;31mValueError\u001b[0m: INTERNAL: Generated function failed: CpuCallback error: RuntimeError: The maximum number of solver steps was reached. Try increasing `max_steps`.\n\nAt:\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/equinox/internal/errors.py(17): raises\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/jax/_src/callback.py(124): _flat_callback\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/jax/_src/callback.py(41): pure_callback_impl\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/jax/_src/callback.py(104): _callback\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/jax/interpreters/mlir.py(1579): _wrapped_callback\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/equinox/jit.py(78): _fun_wrapper\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/equinox/jit.py(82): __call__\n  /var/folders/fm/dr9q5mc50ql8h2wmv7b1fmz80000gn/T/ipykernel_78188/2677578158.py(48): main\n  /var/folders/fm/dr9q5mc50ql8h2wmv7b1fmz80000gn/T/ipykernel_78188/1014090899.py(1): <module>\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3433): run_code\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3373): run_ast_nodes\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py(3194): run_cell_async\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/IPython/core/async_helpers.py(129): _pseudo_sync_runner\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py(2995): _run_cell\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py(2940): run_cell\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/ipykernel/zmqshell.py(528): run_cell\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/ipykernel/ipkernel.py(383): do_execute\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/ipykernel/kernelbase.py(730): execute_request\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/ipykernel/kernelbase.py(406): dispatch_shell\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/ipykernel/kernelbase.py(499): process_one\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/ipykernel/kernelbase.py(510): dispatch_queue\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/asyncio/events.py(80): _run\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/asyncio/base_events.py(1899): _run_once\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/asyncio/base_events.py(603): run_forever\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/tornado/platform/asyncio.py(215): start\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/ipykernel/kernelapp.py(712): start\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/traitlets/config/application.py(846): launch_instance\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/site-packages/ipykernel_launcher.py(17): <module>\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/runpy.py(86): _run_code\n  /Users/roussel/miniforge3/envs/jaxenv/lib/python3.10/runpy.py(196): _run_module_as_main\n"
     ]
    }
   ],
   "source": [
    "ts, ys, model = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7c0626-7e83-4a2b-84c9-44bcfc147f35",
   "metadata": {},
   "source": [
    "Some notes on speed:\n",
    "The hyperparameters for the above example haven't really been optimised. Try experimenting with them to see how much faster you can make this example run. There's lots of things you can try tweaking:\n",
    "- The size of the neural network.\n",
    "- The numerical solver.\n",
    "- The step size controller, including both its step size and its tolerances.\n",
    "- The length of the dataset. (Do you really need to use all of a time series every time?)\n",
    "- Batch size, learning rate, choice of optimiser.\n",
    "- ... etc.!\n",
    "\n",
    "Some notes on being Markov:\n",
    "- This example has assumed that the problem is Markov. Essentially, that the data `ys` is a complete observation of the system, and that we're not missing any channels. Note how the result of our model is evolving in data space. This is unlike e.g. an RNN, which has hidden state, and a linear map from hidden state to data.\n",
    "- If we wanted we could generalise this to the non-Markov case: inside `NeuralODE`, project the initial condition into some high-dimensional latent space, do the ODE solve there, then take a linear map to get the output. See `latent_ode.ipynb` for an example doing this as part of a generative model; also see [Augmented Neural ODEs](https://arxiv.org/abs/1904.01681) for a short paper on it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('jaxenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "8172ebf40a1b5495b14df1ebdc40c339972f6d826997aae164f4bf169121d865"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
