{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Built-in Neural Networks in Φ<sub>Flow</sub>\n",
    "\n",
    "Apart from being a general purpose differential physics library, Φ<sub>Flow</sub> also provides a number of backend-agnostic way of setting up optimizers and some neural networks.\n",
    "\n",
    "The following network architectures are supported:\n",
    "\n",
    "* Fully-connected networks: `dense_net()`\n",
    "* Convolutional networks: `conv_net()`\n",
    "* Residual networks: `res_net()`\n",
    "* U-Nets: `u_net()`\n",
    "* Convolutional classifiers: `conv_classifier()`\n",
    "\n",
    "In addition to zero-padding, the convolutional neural networks all support circular periodic padding across feature map spatial dimensions to maintain periodicity.\n",
    "\n",
    "All network-related convenience functions in Φ<sub>Flow</sub> support PyTorch, TensorFlow and Jax/Stax, setting up native networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from phi.tf.flow import *\n",
    "from phi.jax.stax.flow import *\n",
    "from phi.torch.flow import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Fully-connected Networks\n",
    "Fully-connected neural networks are available in Φ<sub>Flow</sub> via `dense_net()`.\n",
    "\n",
    "#### Arguments\n",
    "\n",
    "* `in_channels` : size of input layer, int\n",
    "* `out_channels` = size of output layer, int\n",
    "* `layers` : tuple of linear layers between input and output neurons, list or tuple\n",
    "* `activation` : activation function used within the layers, string\n",
    "* `batch_norm` : use of batch norm after each linear layer, bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net = dense_net(in_channels=1, out_channels=1, layers=[8, 8], activation='ReLU', batch_norm=False)  # Implemented for PyTorch, TensorFlow, Jax-Stax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: \u001B[92m(batchᵇ=100)\u001B[0m \u001B[94m0.278 ± 0.308\u001B[0m \u001B[37m(4e-05...1e+00)\u001B[0m\n",
      "Final loss: \u001B[92m(batchᵇ=100)\u001B[0m \u001B[94m0.095 ± 0.131\u001B[0m \u001B[37m(1e-05...9e-01)\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "optimizer = adam(net, 1e-3)\n",
    "BATCH = batch(batch=100)\n",
    "\n",
    "def loss_function(data: Tensor):\n",
    "    prediction = math.native_call(net, data)\n",
    "    label = math.sin(data)\n",
    "    return math.l2_loss(prediction - label), data, label\n",
    "\n",
    "print(f\"Initial loss: {loss_function(math.random_normal(BATCH))[0]}\")\n",
    "for i in range(100):\n",
    "    loss, _data, _label = update_weights(net, optimizer, loss_function, data=math.random_normal(BATCH))\n",
    "print(f\"Final loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## U-Nets\n",
    "Φ<sub>Flow</sub> provides a built in U-net architecture, classically popular for Semantic Segmentation in Computer Vision, composed of downsampling and upsampling layers.\n",
    "\n",
    " #### Arguments\n",
    "\n",
    "* `in_channels`: input channels of the feature map, dtype : int\n",
    "* `out_channels` : output channels of the feature map, dtype : int\n",
    "* `levels` : number of levels of down-sampling and upsampling, dtype : int\n",
    "* `filters` : filter sizes at each down/up sampling convolutional layer, if the input is integer all conv layers have the same filter size,<br> dtype : int or tuple\n",
    "* `activation` : activation function used within the layers, dtype : string\n",
    "* `batch_norm` : use of batchnorm after each conv layer, dtype : bool\n",
    "* `in_spatial` : spatial dimensions of the input feature map, dtype : int\n",
    "* `use_res_blocks` : use convolutional blocks with skip connections instead of regular convolutional blocks, dtype : bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net = u_net(in_channels= 1, out_channels= 2, levels=4, filters=16, batch_norm=True, activation='ReLU', in_spatial=2, use_res_blocks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Loss function for training in the network to identify noise parameters\n",
    "def loss_function(scale: Tensor, smoothness: Tensor):\n",
    "    grid = CenteredGrid(Noise(scale=scale, smoothness=smoothness), x=64, y=64)\n",
    "\n",
    "    print(f'Grid Shape : {grid.shape}')\n",
    "    pred_scale, pred_smoothness = field.native_call(net, grid).vector\n",
    "    return math.l2_loss(pred_scale - scale) + math.l2_loss(pred_smoothness - smoothness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Initial loss: \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m9.06e+04 ± 6.6e+04\u001B[0m \u001B[37m(9e+03...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 0, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m9.07e+04 ± 6.6e+04\u001B[0m \u001B[37m(9e+03...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 1, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m8.68e+04 ± 6.0e+04\u001B[0m \u001B[37m(2e+04...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 2, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m8.48e+04 ± 5.6e+04\u001B[0m \u001B[37m(2e+04...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 3, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m8.33e+04 ± 5.4e+04\u001B[0m \u001B[37m(2e+04...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 4, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m8.22e+04 ± 5.3e+04\u001B[0m \u001B[37m(2e+04...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 5, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m8.10e+04 ± 5.1e+04\u001B[0m \u001B[37m(2e+04...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 6, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m8.00e+04 ± 5.0e+04\u001B[0m \u001B[37m(2e+04...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 7, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m7.90e+04 ± 4.9e+04\u001B[0m \u001B[37m(1e+04...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 8, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m7.83e+04 ± 4.8e+04\u001B[0m \u001B[37m(1e+04...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 9, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m7.76e+04 ± 4.8e+04\u001B[0m \u001B[37m(1e+04...2e+05)\u001B[0m\n",
      "Final loss: \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m7.76e+04 ± 4.8e+04\u001B[0m \u001B[37m(1e+04...2e+05)\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "optimizer = adam(net, learning_rate=1e-3)\n",
    "gt_scale = math.random_uniform(batch(examples=50), low=1, high=10)\n",
    "gt_smoothness = math.random_uniform(batch(examples=50), low=.5, high=3)\n",
    "\n",
    "print(f\"Initial loss: {loss_function(gt_scale, gt_smoothness)}\")\n",
    "for i in range(10):\n",
    "    loss = update_weights(net, optimizer, loss_function, gt_scale, gt_smoothness)\n",
    "    print(f'Iter : {i}, Loss : {loss}')\n",
    "print(f\"Final loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Convolutional Networks\n",
    "Built in Conv-Nets are also provided. Contrary to the classical convolutional neural networks, the feature map spatial size remains the same throughout the layers.\n",
    "Each layer of the network is essentially a convolutional block comprising of two conv layers.\n",
    "A filter size of 3 is used in the convolutional layers.\n",
    "\n",
    "#### Arguments\n",
    "\n",
    "* `in_channels` : input channels of the feature map, dtype : int\n",
    "* `out_channels` : output channels of the feature map, dtype : int <br>\n",
    "* `layers` : list or tuple of output channels for each intermediate layer between the input and final output channels, dtype : list or tuple <br>\n",
    "* `activation` : activation function used within the layers, dtype : string <br>\n",
    "* `batch_norm` : use of batchnorm after each conv layer, dtype : bool <br>\n",
    "* `in_spatial` : spatial dimensions of the input feature map, dtype : int <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net = conv_net(in_channels=1, out_channels=2, layers=[2,4,4,2], activation='ReLU', batch_norm=True, in_spatial=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Initial loss: \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m9.76e+04 ± 5.7e+04\u001B[0m \u001B[37m(8e+03...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 0, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m9.77e+04 ± 5.7e+04\u001B[0m \u001B[37m(8e+03...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 1, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m9.73e+04 ± 5.7e+04\u001B[0m \u001B[37m(8e+03...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 2, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m9.71e+04 ± 5.7e+04\u001B[0m \u001B[37m(8e+03...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 3, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m9.68e+04 ± 5.7e+04\u001B[0m \u001B[37m(8e+03...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 4, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m9.65e+04 ± 5.7e+04\u001B[0m \u001B[37m(8e+03...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 5, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m9.61e+04 ± 5.7e+04\u001B[0m \u001B[37m(8e+03...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 6, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m9.58e+04 ± 5.7e+04\u001B[0m \u001B[37m(8e+03...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 7, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m9.55e+04 ± 5.6e+04\u001B[0m \u001B[37m(8e+03...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 8, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m9.52e+04 ± 5.6e+04\u001B[0m \u001B[37m(8e+03...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 9, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m9.48e+04 ± 5.6e+04\u001B[0m \u001B[37m(8e+03...2e+05)\u001B[0m\n",
      "Final loss: \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m9.48e+04 ± 5.6e+04\u001B[0m \u001B[37m(8e+03...2e+05)\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "optimizer = adam(net, learning_rate=1e-3)\n",
    "gt_scale = math.random_uniform(batch(examples=50), low=1, high=10)\n",
    "gt_smoothness = math.random_uniform(batch(examples=50), low=.5, high=3)\n",
    "\n",
    "print(f\"Initial loss: {loss_function(gt_scale, gt_smoothness)}\")\n",
    "for i in range(10):\n",
    "    loss = update_weights(net, optimizer, loss_function, gt_scale, gt_smoothness)\n",
    "    print(f'Iter : {i}, Loss : {loss}')\n",
    "print(f\"Final loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Residual Networks\n",
    "Built in Res-Nets are provided in the Φ<sub>Flow</sub> framework. Similar to the conv-net, the feature map spatial size remains the same throughout the layers.<br>These networks use residual blocks composed of two conv layers with a skip connection added from the input to the output feature map.<br> A default filter size of 3 is used in the convolutional layers.<br><br>\n",
    "\n",
    "#### Arguments\n",
    "\n",
    "* `in_channels` : input channels of the feature map, dtype : int\n",
    "* `out_channels` : output channels of the feature map, dtype : int\n",
    "* `layers` : list or tuple of output channels for each intermediate layer between the input and final output channels, dtype : list or tuple\n",
    "* `activation` : activation function used within the layers, dtype : string\n",
    "* `batch_norm` : use of batchnorm after each conv layer, dtype : bool\n",
    "* `in_spatial` : spatial dimensions of the input feature map, dtype : int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net = res_net(in_channels=1, out_channels=2, layers=[2,4,4,2], activation='ReLU', batch_norm=True, in_spatial=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Initial loss: \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m6.09e+04 ± 5.1e+04\u001B[0m \u001B[37m(1e+04...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 0, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m6.12e+04 ± 5.2e+04\u001B[0m \u001B[37m(1e+04...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 1, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m5.90e+04 ± 4.8e+04\u001B[0m \u001B[37m(1e+04...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 2, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m5.79e+04 ± 4.8e+04\u001B[0m \u001B[37m(1e+04...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 3, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m5.66e+04 ± 4.6e+04\u001B[0m \u001B[37m(1e+04...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 4, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m5.52e+04 ± 4.4e+04\u001B[0m \u001B[37m(1e+04...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 5, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m5.41e+04 ± 4.2e+04\u001B[0m \u001B[37m(1e+04...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 6, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m5.31e+04 ± 4.1e+04\u001B[0m \u001B[37m(1e+04...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 7, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m5.22e+04 ± 4.0e+04\u001B[0m \u001B[37m(1e+04...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 8, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m5.17e+04 ± 3.9e+04\u001B[0m \u001B[37m(1e+04...2e+05)\u001B[0m\n",
      "Grid Shape : (examplesᵇ=50, xˢ=64, yˢ=64)\n",
      "Iter : 9, Loss : \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m5.06e+04 ± 3.7e+04\u001B[0m \u001B[37m(1e+04...1e+05)\u001B[0m\n",
      "Final loss: \u001B[92m(examplesᵇ=50)\u001B[0m \u001B[94m5.06e+04 ± 3.7e+04\u001B[0m \u001B[37m(1e+04...1e+05)\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "optimizer = adam(net, learning_rate=1e-3)\n",
    "gt_scale = math.random_uniform(batch(examples=50), low=1, high=10)\n",
    "gt_smoothness = math.random_uniform(batch(examples=50), low=.5, high=3)\n",
    "\n",
    "print(f\"Initial loss: {loss_function(gt_scale, gt_smoothness)}\")\n",
    "for i in range(10):\n",
    "    loss = update_weights(net, optimizer, loss_function, gt_scale, gt_smoothness)\n",
    "    print(f'Iter : {i}, Loss : {loss}')\n",
    "print(f\"Final loss: {loss}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82902df7c83405ec4dfb07b40523262a4e655c5cd82d0ed637b48934b3b1a8ec"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}